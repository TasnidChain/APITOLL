# Service Configuration
PORT=3001
NODE_ENV=development

# LLM Provider Configuration
# Options: openai, anthropic, local
LLM_PROVIDER=openai

# API Keys (required for openai/anthropic)
# OpenAI: sk-...
# Anthropic: sk-ant-...
LLM_API_KEY=your-api-key-here

# LLM Model Selection
# OpenAI: gpt-4, gpt-4-turbo, gpt-3.5-turbo
# Anthropic: claude-3-opus, claude-3-sonnet, claude-3-haiku
# Local: any local model identifier
LLM_MODEL=gpt-4

# Feature Flags
ENABLE_CACHING=true

# Advanced Configuration
# LLM Request Timeout (milliseconds)
LLM_REQUEST_TIMEOUT=30000

# Default temperature for LLM calls (0.0-2.0)
DEFAULT_TEMPERATURE=0.7

# Default max tokens for LLM responses
DEFAULT_MAX_TOKENS=2000
